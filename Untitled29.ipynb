{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNt7NqNaquy7Sv0WU1/WEgR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aryan151pratap/NLP/blob/main/Untitled29.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BASIC PREPROCESSING"
      ],
      "metadata": {
        "id": "XJO0AaSlwcgv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "n2Q38FOgwSZu"
      },
      "outputs": [],
      "source": [
        "text = 'The Movie was very touching and heart whelming'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lower_case(text):\n",
        "  return text.lower()"
      ],
      "metadata": {
        "id": "l8geTAs7wxGj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lower_case(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "tXu357B6w49W",
        "outputId": "bd930832-f5d6-4c79-ca83-89e6b6ade090"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the movie was very touching and heart whelming'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Tokenization\n",
        "\n",
        "    1. split function\n",
        "    2. Regex\n",
        "    3."
      ],
      "metadata": {
        "id": "BpB48333xTGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent1 = 'Hey i am going to delhi'\n",
        "sent1.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1x6aB8S4xG2p",
        "outputId": "857c54e4-6cc9-4a54-bf63-800d74bd4adf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hey', 'i', 'am', 'going', 'to', 'delhi']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent2 = 'Hey i am going to delhi, will you help me out'\n",
        "sent2.split(',')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNbEsonqxxaO",
        "outputId": "ce0efe5f-4620-4ac3-d300-776d5785d634"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hey i am going to delhi', ' will you help me out']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent3 = 'hey, new delhi is the capital of india'\n",
        "sent3.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2TqJAreyDY7",
        "outputId": "763a2b58-48a6-42e7-8b87-258d74ff344a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hey,', 'new', 'delhi', 'is', 'the', 'capital', 'of', 'india']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent4 = 'hey i am going to delhi'\n",
        "sent4.split('.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmA4j1bOySks",
        "outputId": "280b912e-87ae-4c9f-fa89-bf7715cec4c5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hey i am going to delhi']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "sent5 = 'i am going to delhi'\n",
        "tokens = re.findall(r'\\w+', sent5)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGW4Md_4yiHP",
        "outputId": "f3e3ca5a-9013-4dc3-c234-437685d6e749"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'am', 'going', 'to', 'delhi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "One rainy evening, Aarav found an old key lying near the gate of his college. Curious, he slipped it into his pocket. The next day, while wandering through the library’s dusty basement, he noticed a small, rusty door he had never seen before.\n",
        "On impulse, he tried the key. The lock clicked open.\n",
        "Inside was a forgotten room filled with handwritten journals, maps, and strange sketches of machines no one had ever built. A note on the desk read:\n",
        "\"For the one who dares to discover, the future is yours to create.\"\n",
        "From that day, Aarav visited the secret room every evening, learning from the ideas left behind, determined to finish what the unknown dreamer had started.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "3VzpvKfYzArc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = re.compile('[.!,?\"] ').split(text)\n",
        "sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zkw4n2-SzVwn",
        "outputId": "4bd35858-143d-4ac5-862a-1977dac22cc9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\nOne rainy evening',\n",
              " 'Aarav found an old key lying near the gate of his college',\n",
              " 'Curious',\n",
              " 'he slipped it into his pocket',\n",
              " 'The next day',\n",
              " 'while wandering through the library’s dusty basement',\n",
              " 'he noticed a small',\n",
              " 'rusty door he had never seen before.\\nOn impulse',\n",
              " 'he tried the key',\n",
              " 'The lock clicked open.\\nInside was a forgotten room filled with handwritten journals',\n",
              " 'maps',\n",
              " 'and strange sketches of machines no one had ever built',\n",
              " 'A note on the desk read:\\n\"For the one who dares to discover',\n",
              " 'the future is yours to create.\"\\nFrom that day',\n",
              " 'Aarav visited the secret room every evening',\n",
              " 'learning from the ideas left behind',\n",
              " 'determined to finish what the unknown dreamer had started.\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGkonayY1Sdi",
        "outputId": "b6e03f3c-73ed-45f5-bfec-38f162bef641"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ],
      "metadata": {
        "id": "7uGX5sr60Ej8"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent1 = 'I, am going to Delhi'\n",
        "tokens = word_tokenize(sent1)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5t6D3CA0zCr",
        "outputId": "49b85fb3-18ef-49fb-e3c0-0cc10b8b6793"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', ',', 'am', 'going', 'to', 'Delhi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent6 = 'i have email id as james.madison@examplepetstore.com www.google.com'\n",
        "tokens = word_tokenize(sent6)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KV_msoMS037g",
        "outputId": "522426c9-6c66-4ee9-b646-395eec774195"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'have', 'email', 'id', 'as', 'james.madison', '@', 'examplepetstore.com', 'www.google.com']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary = \"For the one who dares to discover, the future is yours to create.\"\n",
        "sent_tokenize(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ug3KE8E2B8t",
        "outputId": "a9892cc7-1868-4084-9d36-7d15a395124a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['For the one who dares to discover, the future is yours to create.']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SPACY**"
      ],
      "metadata": {
        "id": "JaMhLJ3q2xdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "TQnTbn3K2qJv"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc1 = nlp(summary)"
      ],
      "metadata": {
        "id": "-OgvDCBW3WII"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sent in doc1.sents:\n",
        "  print(sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1TcDb4a3iQz",
        "outputId": "e2194b9d-3b8b-4a66-a3ae-db99c7433732"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For the one who dares to discover, the future is yours to create.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Stop Word Removel"
      ],
      "metadata": {
        "id": "gF0XxIL34Xqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPQztrxS3t_w",
        "outputId": "434d5a89-ada6-4066-9169-7956a68953a5"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "uzLD_nRC4kos"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('french')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBr2tkTT4qcb",
        "outputId": "44523670-210d-47a5-aed6-44aac24a6349"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['au',\n",
              " 'aux',\n",
              " 'avec',\n",
              " 'ce',\n",
              " 'ces',\n",
              " 'dans',\n",
              " 'de',\n",
              " 'des',\n",
              " 'du',\n",
              " 'elle',\n",
              " 'en',\n",
              " 'et',\n",
              " 'eux',\n",
              " 'il',\n",
              " 'ils',\n",
              " 'je',\n",
              " 'la',\n",
              " 'le',\n",
              " 'les',\n",
              " 'leur',\n",
              " 'lui',\n",
              " 'ma',\n",
              " 'mais',\n",
              " 'me',\n",
              " 'même',\n",
              " 'mes',\n",
              " 'moi',\n",
              " 'mon',\n",
              " 'ne',\n",
              " 'nos',\n",
              " 'notre',\n",
              " 'nous',\n",
              " 'on',\n",
              " 'ou',\n",
              " 'par',\n",
              " 'pas',\n",
              " 'pour',\n",
              " 'qu',\n",
              " 'que',\n",
              " 'qui',\n",
              " 'sa',\n",
              " 'se',\n",
              " 'ses',\n",
              " 'son',\n",
              " 'sur',\n",
              " 'ta',\n",
              " 'te',\n",
              " 'tes',\n",
              " 'toi',\n",
              " 'ton',\n",
              " 'tu',\n",
              " 'un',\n",
              " 'une',\n",
              " 'vos',\n",
              " 'votre',\n",
              " 'vous',\n",
              " 'c',\n",
              " 'd',\n",
              " 'j',\n",
              " 'l',\n",
              " 'à',\n",
              " 'm',\n",
              " 'n',\n",
              " 's',\n",
              " 't',\n",
              " 'y',\n",
              " 'été',\n",
              " 'étée',\n",
              " 'étées',\n",
              " 'étés',\n",
              " 'étant',\n",
              " 'étante',\n",
              " 'étants',\n",
              " 'étantes',\n",
              " 'suis',\n",
              " 'es',\n",
              " 'est',\n",
              " 'sommes',\n",
              " 'êtes',\n",
              " 'sont',\n",
              " 'serai',\n",
              " 'seras',\n",
              " 'sera',\n",
              " 'serons',\n",
              " 'serez',\n",
              " 'seront',\n",
              " 'serais',\n",
              " 'serait',\n",
              " 'serions',\n",
              " 'seriez',\n",
              " 'seraient',\n",
              " 'étais',\n",
              " 'était',\n",
              " 'étions',\n",
              " 'étiez',\n",
              " 'étaient',\n",
              " 'fus',\n",
              " 'fut',\n",
              " 'fûmes',\n",
              " 'fûtes',\n",
              " 'furent',\n",
              " 'sois',\n",
              " 'soit',\n",
              " 'soyons',\n",
              " 'soyez',\n",
              " 'soient',\n",
              " 'fusse',\n",
              " 'fusses',\n",
              " 'fût',\n",
              " 'fussions',\n",
              " 'fussiez',\n",
              " 'fussent',\n",
              " 'ayant',\n",
              " 'ayante',\n",
              " 'ayantes',\n",
              " 'ayants',\n",
              " 'eu',\n",
              " 'eue',\n",
              " 'eues',\n",
              " 'eus',\n",
              " 'ai',\n",
              " 'as',\n",
              " 'avons',\n",
              " 'avez',\n",
              " 'ont',\n",
              " 'aurai',\n",
              " 'auras',\n",
              " 'aura',\n",
              " 'aurons',\n",
              " 'aurez',\n",
              " 'auront',\n",
              " 'aurais',\n",
              " 'aurait',\n",
              " 'aurions',\n",
              " 'auriez',\n",
              " 'auraient',\n",
              " 'avais',\n",
              " 'avait',\n",
              " 'avions',\n",
              " 'aviez',\n",
              " 'avaient',\n",
              " 'eut',\n",
              " 'eûmes',\n",
              " 'eûtes',\n",
              " 'eurent',\n",
              " 'aie',\n",
              " 'aies',\n",
              " 'ait',\n",
              " 'ayons',\n",
              " 'ayez',\n",
              " 'aient',\n",
              " 'eusse',\n",
              " 'eusses',\n",
              " 'eût',\n",
              " 'eussions',\n",
              " 'eussiez',\n",
              " 'eussent']"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text):\n",
        "  new_text = []\n",
        "  for words in text.split():\n",
        "    if words in stopwords.words('english'):\n",
        "      new_text.append('')\n",
        "    else:\n",
        "      new_text.append(words)\n",
        "\n",
        "  x = new_text[:]\n",
        "  new_text.clear()\n",
        "  return \" \".join(x)\n"
      ],
      "metadata": {
        "id": "CAn5-0yi4t3T"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Inside was a forgotten room filled with handwritten journals, maps, and strange sketches of machines no one had ever built. A note on the desk read:'\n",
        "\n",
        "remove_stopwords(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "mN3VFRY66Oxt",
        "outputId": "8c600bc0-ffd0-41d7-b1dd-7cdc08b4be39"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Inside   forgotten room filled  handwritten journals, maps,  strange sketches  machines  one  ever built. A note   desk read:'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_words = {\n",
        "    'AFAIK':'As Far As I Know',\n",
        "    'AFK':'Away From Keyboard',\n",
        "    'ASAP':'As Soon As Possible',\n",
        "    'ATK':'At The Keyboard',\n",
        "    'ATM':'At The Moment',\n",
        "}"
      ],
      "metadata": {
        "id": "g5Jfm7-A78L_"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_conv(text):\n",
        "  new_text= []\n",
        "  for word in text.split():\n",
        "    if word.upper() in chat_words:\n",
        "      new_text.append(chat_words[word.upper()])\n",
        "    else:\n",
        "      new_text.append(word)\n",
        "\n",
        "  return \" \".join(new_text)"
      ],
      "metadata": {
        "id": "f4aTBa-66sEZ"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_conv(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "F5u5I-vX8GcJ",
        "outputId": "81139961-721e-47ab-b17d-34e8fc195527"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Inside was a forgotten room filled with handwritten journals, maps, and strange sketches of machines no one had ever built. A note on the desk read:'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming -> give root word -> apart from dictonary may be having some meaning\n",
        "# lamatization -> give root word -> from dictonary having some meaning"
      ],
      "metadata": {
        "id": "lHiRoKIt8JxH"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Stemming\n",
        "Stemming -> give root word -> apart from dictonary may be having some meaning or no meaning\n"
      ],
      "metadata": {
        "id": "TQVZ7FuE9jDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "s-9dwiEo8pJW"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()\n",
        "def stem_words(text):\n",
        "  return \" \".join([ps.stem(word) for word in text.split()])"
      ],
      "metadata": {
        "id": "HCEQNtgC8rfA"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text= \"The sun dipped below the horizon, painting the sky in shades of orange and purple. A cool breeze rustled through the trees as Riya sat by the lake, watching the ripples dance across the water. For a moment, the world felt quiet and unhurried, as if time itself had paused to breathe.\""
      ],
      "metadata": {
        "id": "R43Ymgls9Ni4"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stem_words(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "9di40wKq9Pw9",
        "outputId": "63b2ebc3-8032-42c1-869c-571192e90a88"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the sun dip below the horizon, paint the sky in shade of orang and purple. a cool breez rustl through the tree as riya sat by the lake, watch the rippl danc across the water. for a moment, the world felt quiet and unhurried, as if time itself had paus to breathe.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Lemmatization\n",
        "lamatization -> give root word -> from dictonary having some meaning"
      ],
      "metadata": {
        "id": "B6eR3Bnu9vzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzhZu8jx9UXK",
        "outputId": "75fd72bd-253a-41a0-b24b-04c793de1dd2"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "JeaXhJJb98bK"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "\n",
        "sentence = \"The sun dipped below the horizon, painting the sky in shades of orange and purple. A cool breeze rustled through the trees as Riya sat by the lake, watching the ripples dance across the water. For a moment, the world felt quiet and unhurried, as if time itself had paused to breathe.\"\n",
        "punctuation = '?:|.,;'\n",
        "sentence_words = nltk.word_tokenize(sentence)\n",
        "for word in sentence_words:\n",
        "    if word in punctuation:\n",
        "        sentence_words.remove(word)\n",
        "\n",
        "sentence_words\n",
        "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
        "for word in sentence_words:\n",
        "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZVpSfv--K15",
        "outputId": "718372f6-6c4c-4a0d-fecd-5e5876227b59"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word                Lemma               \n",
            "The                 The                 \n",
            "sun                 sun                 \n",
            "dipped              dipped              \n",
            "below               below               \n",
            "the                 the                 \n",
            "horizon             horizon             \n",
            "painting            painting            \n",
            "the                 the                 \n",
            "sky                 sky                 \n",
            "in                  in                  \n",
            "shades              shade               \n",
            "of                  of                  \n",
            "orange              orange              \n",
            "and                 and                 \n",
            "purple              purple              \n",
            "A                   A                   \n",
            "cool                cool                \n",
            "breeze              breeze              \n",
            "rustled             rustled             \n",
            "through             through             \n",
            "the                 the                 \n",
            "trees               tree                \n",
            "as                  a                   \n",
            "Riya                Riya                \n",
            "sat                 sat                 \n",
            "by                  by                  \n",
            "the                 the                 \n",
            "lake                lake                \n",
            "watching            watching            \n",
            "the                 the                 \n",
            "ripples             ripple              \n",
            "dance               dance               \n",
            "across              across              \n",
            "the                 the                 \n",
            "water               water               \n",
            "For                 For                 \n",
            "a                   a                   \n",
            "moment              moment              \n",
            "the                 the                 \n",
            "world               world               \n",
            "felt                felt                \n",
            "quiet               quiet               \n",
            "and                 and                 \n",
            "unhurried           unhurried           \n",
            "as                  a                   \n",
            "if                  if                  \n",
            "time                time                \n",
            "itself              itself              \n",
            "had                 had                 \n",
            "paused              paused              \n",
            "to                  to                  \n",
            "breathe             breathe             \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Laguage detection"
      ],
      "metadata": {
        "id": "I7oemfGp_arQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tH3iZoB4-ZMl",
        "outputId": "c17500c5-97cc-4a5f-e44a-53ec01a58fb2"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.0/981.5 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from langdetect) (1.17.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=46822771de08fd26ea8f220b6ffd446e22959940a8b3bdd1475202d2a606a92a\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/67/88/e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text= \"The sun dipped below the horizon, painting the sky in shades of orange and purple. A cool breeze rustled through the trees as Riya sat by the lake, watching the ripples dance across the water. For a moment, the world felt quiet and unhurried, as if time itself had paused to breathe.\""
      ],
      "metadata": {
        "id": "tr-tRI4__2Bw"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langdetect import detect, detect_langs\n",
        "def detect_laguage(text):\n",
        "  try:\n",
        "    lang = detect(text)\n",
        "    lang_proh = detect_langs(text)\n",
        "    return lang, lang_proh\n",
        "  except Exception as e:\n",
        "    return 'Error', str(e)"
      ],
      "metadata": {
        "id": "kl1hidcb_ott"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detect_laguage(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2iYnXhUAl5q",
        "outputId": "2a95a600-9df9-45dd-c1a9-55afb1469ea3"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('en', [en:0.9999968211904058])"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "POS Lagging"
      ],
      "metadata": {
        "id": "hhlUU3DdCPO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tex = 'the delhi is the capital of india'"
      ],
      "metadata": {
        "id": "2v1zspB7AoFu"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "id": "CtFAPU4uCf8S",
        "outputId": "21ec3498-99e2-48e3-d6c4-2be1e9039ec0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "def get_nltk(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged = pos_tag(tokens)\n",
        "    return tagged\n",
        "\n",
        "sentence = \"I am going to Delhi\"\n",
        "print(get_nltk(sentence))"
      ],
      "metadata": {
        "id": "_-zSbKo0DCWr",
        "outputId": "dc0e4f56-f55b-4723-bf32-cf6f79308079",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', 'PRP'), ('am', 'VBP'), ('going', 'VBG'), ('to', 'TO'), ('Delhi', 'VB')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'i am going to delhi'\n",
        "new_text = get_nltk(text)\n",
        "print(new_text)"
      ],
      "metadata": {
        "id": "KVPM6e4SDXrP",
        "outputId": "2d83c512-7d55-45ca-c58e-a79261d45e5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('i', 'NN'), ('am', 'VBP'), ('going', 'VBG'), ('to', 'TO'), ('delhi', 'VB')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "id": "ujXlX1KcD_GO",
        "outputId": "887e3331-4eb5-45d8-eb91-25147d04cc09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I am going to Delhi.\"\n",
        "\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def parse_sentences(text):\n",
        "  doc = nlp(text)\n",
        "  for token in doc:\n",
        "      print(token.text, \"->\", token.pos_, \"|\", token.tag_)\n"
      ],
      "metadata": {
        "id": "C7pvigYKEaOW"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parse_sentences(text)"
      ],
      "metadata": {
        "id": "EyIS-6P0Fi4O",
        "outputId": "c13a4947-f0ee-4f56-b06e-e1fad4940243",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I -> PRON | PRP\n",
            "am -> AUX | VBP\n",
            "going -> VERB | VBG\n",
            "to -> ADP | IN\n",
            "Delhi -> PROPN | NNP\n",
            ". -> PUNCT | .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "coreferencd resolution\n",
        "\n",
        "ex -> ram is good god. he is strong.\n",
        "\n",
        "ram, he -> same person (how to deal with this type text -> *corferenced resolution*)"
      ],
      "metadata": {
        "id": "tdmnSb-VFDBu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VOdC0lk6E2rU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}